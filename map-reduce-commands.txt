##Command to execute first map-reduce job
hadoop jar hadoop-streaming-2.7.3.jar -files mapper1.py,reducer1.py -mapper 'python mapper1.py' -reducer 'python reducer1.py' -input /user/hive/assignment1/ -output /user/mapred/output_1

##Command to execute second map-reduce job
hadoop jar hadoop-streaming-2.7.3.jar -files mapper2.py,reducer2.py -mapper 'python mapper2.py' -reducer 'python reducer2.py' -input /user/mapred/output_1/ -output /user/mapred/output_2

##Command to execute third map-reduce job
hadoop jar hadoop-streaming-2.7.3.jar -files mapper3.py,reducer3.py -mapper 'python mapper3.py' -reducer 'python reducer3.py' -input /user/mapred/output_2/ -output /user/mapred/output_3

##Command to execute fourth mapper job

hadoop jar hadoop-streaming-2.7.3.jar -files mapper4.py -mapper 'python mapper4.py' -numReduceTasks 0 -input /user/mapred/output_3/ -output /user/mapred/output_4

# Use the below command to combine the result of mapper-reducer job
hdfs dfs -cat /user/mapred/output_4/part-* >> CombinedResult.txt
